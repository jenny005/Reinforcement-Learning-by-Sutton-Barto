{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmUbEzt9W7GhiKzIIYwZlB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenny005/Reinforcement-Learning-by-Sutton-Barto/blob/main/Chapter_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 3 : Finite Markov Decision Processes"
      ],
      "metadata": {
        "id": "-inYzFLVYAVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 3.5 Gridworld"
      ],
      "metadata": {
        "id": "zF1zVBc_4WbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# modify from\n",
        "#######################################################################\n",
        "# Copyright (C)                                                       #\n",
        "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
        "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
        "# Permission given to modify the code as long as you keep this        #\n",
        "# declaration at the top                                              #\n",
        "#######################################################################\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Gridworld Setup (Figure 3.2) ---\n",
        "WORLD_SIZE = 5\n",
        "DISCOUNT = 0.9\n",
        "ACTION_PROB = 0.25\n",
        "\n",
        "A_POS = (0, 1)\n",
        "A_PRIME_POS = (4, 1)\n",
        "B_POS = (0, 3)\n",
        "B_PRIME_POS = (2, 3)\n",
        "\n",
        "ACTIONS = [\n",
        "    np.array([0, -1]),  # Left\n",
        "    np.array([-1, 0]),  # Up\n",
        "    np.array([0, 1]),   # Right\n",
        "    np.array([1, 0])    # Down\n",
        "]\n",
        "\n",
        "def step(state, action):\n",
        "    \"\"\"Transition dynamics following Figure 3.2 logic.\"\"\"\n",
        "    if tuple(state) == A_POS:\n",
        "        return A_PRIME_POS, 10\n",
        "    if tuple(state) == B_POS:\n",
        "        return B_PRIME_POS, 5\n",
        "\n",
        "    next_state = np.array(state) + action\n",
        "    if 0 <= next_state[0] < WORLD_SIZE and 0 <= next_state[1] < WORLD_SIZE:\n",
        "        return tuple(next_state), 0  # valid move\n",
        "    else:\n",
        "        return tuple(state), -1      # wall hit\n",
        "\n",
        "def evaluate_uniform_random_policy():\n",
        "    \"\"\"Evaluate uniform random policy using iterative policy evaluation.\"\"\"\n",
        "    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
        "    while True:\n",
        "        new_value = np.zeros_like(value)\n",
        "        for i in range(WORLD_SIZE):\n",
        "            for j in range(WORLD_SIZE):\n",
        "                v = 0\n",
        "                for action in ACTIONS:\n",
        "                    (ni, nj), reward = step((i, j), action)\n",
        "                    v += ACTION_PROB * (reward + DISCOUNT * value[ni, nj])\n",
        "                new_value[i, j] = v\n",
        "        if np.sum(np.abs(new_value - value)) < 1e-4:\n",
        "            break\n",
        "        value = new_value\n",
        "    return np.round(value, decimals=1)\n",
        "\n",
        "def format_value_grid(value_grid):\n",
        "    \"\"\"Formats the grid for clean display.\"\"\"\n",
        "    return [[\"{:+.1f}\".format(v) for v in row] for row in value_grid]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    value_grid = evaluate_uniform_random_policy()\n",
        "    formatted_grid = format_value_grid(value_grid)\n",
        "\n",
        "    # Display as a pandas DataFrame\n",
        "    df = pd.DataFrame(formatted_grid)\n",
        "    print(\"=== Value Function: Uniform Random Policy (Figure 3.2) ===\")\n",
        "    print(df.to_string(index=False, header=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z2YYxytVng5",
        "outputId": "dda00701-1a92-4890-83ae-04b5ce9adbb4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Value Function: Uniform Random Policy (Figure 3.2) ===\n",
            "+3.3 +8.8 +4.4 +5.3 +1.5\n",
            "+1.5 +3.0 +2.3 +1.9 +0.5\n",
            "+0.1 +0.7 +0.7 +0.4 -0.4\n",
            "-1.0 -0.4 -0.4 -0.6 -1.2\n",
            "-1.9 -1.3 -1.2 -1.4 -2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# modify from\n",
        "#######################################################################\n",
        "# Copyright (C)                                                       #\n",
        "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
        "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
        "# Permission given to modify the code as long as you keep this        #\n",
        "# declaration at the top                                              #\n",
        "#######################################################################\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Gridworld Setup (Figure 3.5) ---\n",
        "WORLD_SIZE = 5\n",
        "DISCOUNT = 0.9\n",
        "A_POS = (0, 1)\n",
        "A_PRIME_POS = (4, 1)\n",
        "B_POS = (0, 3)\n",
        "B_PRIME_POS = (2, 3)\n",
        "\n",
        "ACTIONS = [\n",
        "    np.array([0, -1]),  # Left\n",
        "    np.array([-1, 0]),  # Up\n",
        "    np.array([0, 1]),   # Right\n",
        "    np.array([1, 0])    # Down\n",
        "]\n",
        "ACTION_SYMBOLS = ['←', '↑', '→', '↓']\n",
        "\n",
        "def step(state, action):\n",
        "    if tuple(state) == A_POS:\n",
        "        return A_PRIME_POS, 10\n",
        "    if tuple(state) == B_POS:\n",
        "        return B_PRIME_POS, 5\n",
        "\n",
        "    next_state = np.array(state) + action\n",
        "    if 0 <= next_state[0] < WORLD_SIZE and 0 <= next_state[1] < WORLD_SIZE:\n",
        "        return tuple(next_state), 0\n",
        "    else:\n",
        "        return tuple(state), -1\n",
        "\n",
        "def value_iteration(theta=1e-4):\n",
        "    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
        "    while True:\n",
        "        delta = 0\n",
        "        new_value = np.copy(value)\n",
        "        for i in range(WORLD_SIZE):\n",
        "            for j in range(WORLD_SIZE):\n",
        "                v_list = []\n",
        "                for action in ACTIONS:\n",
        "                    (ni, nj), reward = step((i, j), action)\n",
        "                    v = reward + DISCOUNT * value[ni, nj]\n",
        "                    v_list.append(v)\n",
        "                best_v = max(v_list)\n",
        "                new_value[i, j] = best_v\n",
        "                delta = max(delta, abs(best_v - value[i, j]))\n",
        "        value = new_value\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return np.round(value, 1)\n",
        "\n",
        "def extract_policy(value):\n",
        "    policy_grid = [['' for _ in range(WORLD_SIZE)] for _ in range(WORLD_SIZE)]\n",
        "    for i in range(WORLD_SIZE):\n",
        "        for j in range(WORLD_SIZE):\n",
        "            v_list = []\n",
        "            for idx, action in enumerate(ACTIONS):\n",
        "                (ni, nj), reward = step((i, j), action)\n",
        "                v = reward + DISCOUNT * value[ni, nj]\n",
        "                v_list.append((v, idx))\n",
        "            max_v = max(v_list)[0]\n",
        "            best_actions = [ACTION_SYMBOLS[idx] for v, idx in v_list if v == max_v]\n",
        "            policy_grid[i][j] = ''.join(best_actions)\n",
        "    return policy_grid\n",
        "\n",
        "def print_grid(grid, title):\n",
        "    print(f\"\\n=== {title} ===\")\n",
        "    for row in grid:\n",
        "        print(' '.join(str(x).rjust(4) for x in row))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    optimal_value = value_iteration()\n",
        "    optimal_policy = extract_policy(optimal_value)\n",
        "\n",
        "    print_grid(optimal_value, \"Optimal Value Function (Figure 3.5)\")\n",
        "    print_grid(optimal_policy, \"Optimal Policy (Arrows)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuVzVLueWHVZ",
        "outputId": "f2d39cfc-cf65-4542-f14d-36e2e660601d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Optimal Value Function (Figure 3.5) ===\n",
            "22.0 24.4 22.0 19.4 17.5\n",
            "19.8 22.0 19.8 17.8 16.0\n",
            "17.8 19.8 17.8 16.0 14.4\n",
            "16.0 17.8 16.0 14.4 13.0\n",
            "14.4 16.0 14.4 13.0 11.7\n",
            "\n",
            "=== Optimal Policy (Arrows) ===\n",
            "   → ←↑→↓    ← ←↑→↓    ←\n",
            "  ↑→    ↑   ←↑    ←    ←\n",
            "  ↑→    ↑   ←↑   ←↑   ←↑\n",
            "  ↑→    ↑   ←↑   ←↑   ←↑\n",
            "  ↑→    ↑   ←↑   ←↑   ←↑\n"
          ]
        }
      ]
    }
  ]
}