{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSamrZzqgRNpSNSMJ0RAV/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenny005/Reinforcement-Learning-by-Sutton-Barto/blob/main/Chapter_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 3 : Finite Markov Decision Processes"
      ],
      "metadata": {
        "id": "-inYzFLVYAVG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGNEMZsPXV3c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gridworld"
      ],
      "metadata": {
        "id": "zF1zVBc_4WbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridworldEnv:\n",
        "    def __init__(self, height=5, width=5):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.action_space = [0, 1, 2, 3]  # 0: Up, 1: Down, 2: Left, 3: Right\n",
        "        self.A = (0, 1)\n",
        "        self.A_prime = (4, 1)\n",
        "        self.B = (0, 3)\n",
        "        self.B_prime = (2, 3)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        i, j = state\n",
        "\n",
        "        # Special teleportation states\n",
        "        if state == self.A:\n",
        "            return self.A_prime, 10\n",
        "        elif state == self.B:\n",
        "            return self.B_prime, 5\n",
        "\n",
        "        # Action to direction mapping\n",
        "        directions = {\n",
        "            0: (-1, 0),  # Up\n",
        "            1: (1, 0),   # Down\n",
        "            2: (0, -1),  # Left\n",
        "            3: (0, 1)    # Right\n",
        "        }\n",
        "\n",
        "        di, dj = directions[action]\n",
        "        ni, nj = i + di, j + dj\n",
        "\n",
        "        # Check boundary conditions\n",
        "        if 0 <= ni < self.height and 0 <= nj < self.width:\n",
        "            return (ni, nj), 0\n",
        "        else:\n",
        "            return (i, j), -1\n",
        "\n",
        "    def get_all_states(self):\n",
        "        return [(i, j) for i in range(self.height) for j in range(self.width)]\n",
        "\n",
        "    def get_available_actions(self, state):\n",
        "        return self.action_space\n"
      ],
      "metadata": {
        "id": "jpW0PrRh4Xjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridworldEnv()\n",
        "\n",
        "print(\"From A:\", env.step(env.A, 0))          # Expected: ((4, 1), 10)\n",
        "print(\"From B:\", env.step(env.B, 1))          # Expected: ((2, 3), 5)\n",
        "print(\"From (0, 0) going up:\", env.step((0, 0), 0))  # Expected: ((0, 0), -1)\n",
        "print(\"From (2, 2) going right:\", env.step((2, 2), 3))  # Expected: ((2, 3), 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnPdtjsY5VaC",
        "outputId": "1364e54f-4457-425f-9e81-f603460e0eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From A: ((4, 1), 10)\n",
            "From B: ((2, 3), 5)\n",
            "From (0, 0) going up: ((0, 0), -1)\n",
            "From (2, 2) going right: ((2, 3), 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the Gridworld\n",
        "grid_size = 4\n",
        "states = [(i, j) for i in range(grid_size) for j in range(grid_size)]\n",
        "actions = ['U', 'D', 'L', 'R']  # Up, Down, Left, Right\n",
        "gamma = 1.0\n",
        "theta = 1e-4  # Convergence threshold\n",
        "\n",
        "# Transition dynamics\n",
        "def step(state, action):\n",
        "    i, j = state\n",
        "    if state == (0, 0) or state == (3, 3):\n",
        "        return state, 0  # Terminal state\n",
        "\n",
        "    if action == 'U':\n",
        "        i = max(i - 1, 0)\n",
        "    elif action == 'D':\n",
        "        i = min(i + 1, grid_size - 1)\n",
        "    elif action == 'L':\n",
        "        j = max(j - 1, 0)\n",
        "    elif action == 'R':\n",
        "        j = min(j + 1, grid_size - 1)\n",
        "\n",
        "    return (i, j), -1\n",
        "\n",
        "# Initialize value function\n",
        "V = np.zeros((grid_size, grid_size))\n",
        "\n",
        "# Iterative Policy Evaluation\n",
        "iteration = 0\n",
        "while iteration<10:\n",
        "    delta = 0\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            state = (i, j)\n",
        "            if state == (0, 0) or state == (3, 3):\n",
        "                continue  # Skip terminal states\n",
        "\n",
        "            v = V[i, j]\n",
        "            new_v = 0\n",
        "            for action in actions:\n",
        "                next_state, reward = step(state, action)\n",
        "                ni, nj = next_state\n",
        "                new_v += 0.25 * (reward + gamma * V[ni, nj])  # uniform policy\n",
        "\n",
        "            V[i, j] = new_v\n",
        "            delta = max(delta, abs(v - new_v))\n",
        "    iteration += 1\n",
        "    if delta < theta:\n",
        "        break\n",
        "\n",
        "    # Print the final value function\n",
        "    print(f\"Converged in {iteration} iterations.\\n\")\n",
        "    print(\"Optimal state values under uniform policy:\\n\")\n",
        "    for row in V:\n",
        "        print([\"{:+.2f}\".format(val) for val in row])\n",
        "\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "id": "5vEtckIGY_4M",
        "outputId": "6911efd7-f761-4945-fa3a-001e1dc6de5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged in 1 iterations.\n",
            "\n",
            "Optimal state values under uniform policy:\n",
            "\n",
            "['+0.00', '-1.00', '-1.25', '-1.31']\n",
            "['-1.00', '-1.50', '-1.69', '-1.75']\n",
            "['-1.25', '-1.69', '-1.84', '-1.90']\n",
            "['-1.31', '-1.75', '-1.90', '+0.00']\n",
            "--------------------------------------------------\n",
            "Converged in 2 iterations.\n",
            "\n",
            "Optimal state values under uniform policy:\n",
            "\n",
            "['+0.00', '-1.94', '-2.55', '-2.73']\n",
            "['-1.94', '-2.81', '-3.24', '-3.40']\n",
            "['-2.55', '-3.24', '-3.57', '-3.22']\n",
            "['-2.73', '-3.40', '-3.22', '+0.00']\n",
            "--------------------------------------------------\n",
            "Converged in 3 iterations.\n",
            "\n",
            "Optimal state values under uniform policy:\n",
            "\n",
            "['+0.00', '-2.82', '-3.83', '-4.18']\n",
            "['-2.82', '-4.03', '-4.71', '-4.88']\n",
            "['-3.83', '-4.71', '-4.96', '-4.26']\n",
            "['-4.18', '-4.88', '-4.26', '+0.00']\n",
            "--------------------------------------------------\n",
            "Converged in 4 iterations.\n",
            "\n",
            "Optimal state values under uniform policy:\n",
            "\n",
            "['+0.00', '-3.67', '-5.10', '-5.58']\n",
            "['-3.67', '-5.19', '-6.03', '-6.19']\n",
            "['-5.10', '-6.03', '-6.15', '-5.15']\n",
            "['-5.58', '-6.19', '-5.15', '+0.00']\n",
            "--------------------------------------------------\n",
            "Converged in 5 iterations.\n",
            "\n",
            "Optimal state values under uniform policy:\n",
            "\n",
            "['+0.00', '-4.49', '-6.30', '-6.91']\n",
            "['-4.49', '-6.26', '-7.22', '-7.37']\n",
            "['-6.30', '-7.22', '-7.19', '-5.93']\n",
            "['-6.91', '-7.37', '-5.93', '+0.00']\n",
            "--------------------------------------------------\n",
            "Converged in 6 iterations.\n",
            "\n",
            "Optimal state values under uniform policy:\n",
            "\n",
            "['+0.00', '-5.26', '-7.43', '-8.16']\n",
            "['-5.26', '-7.24', '-8.31', '-8.44']\n",
            "['-7.43', '-8.31', '-8.12', '-6.62']\n",
            "['-8.16', '-8.44', '-6.62', '+0.00']\n",
            "--------------------------------------------------\n",
            "Converged in 7 iterations.\n",
            "\n",
            "Optimal state values under uniform policy:\n",
            "\n",
            "['+0.00', '-5.98', '-8.47', '-9.30']\n",
            "['-5.98', '-8.14', '-9.29', '-9.41']\n",
            "['-8.47', '-9.29', '-8.96', '-7.25']\n",
            "['-9.30', '-9.41', '-7.25', '+0.00']\n",
            "--------------------------------------------------\n",
            "Converged in 8 iterations.\n",
            "\n",
            "Optimal state values under uniform policy:\n",
            "\n",
            "['+0.00', '-6.65', '-9.43', '-10.36']\n",
            "['-6.65', '-8.97', '-10.19', '-10.30']\n",
            "['-9.43', '-10.19', '-9.72', '-7.82']\n",
            "['-10.36', '-10.30', '-7.82', '+0.00']\n",
            "--------------------------------------------------\n",
            "Converged in 9 iterations.\n",
            "\n",
            "Optimal state values under uniform policy:\n",
            "\n",
            "['+0.00', '-7.26', '-10.31', '-11.34']\n",
            "['-7.26', '-9.73', '-11.02', '-11.12']\n",
            "['-10.31', '-11.02', '-10.42', '-8.34']\n",
            "['-11.34', '-11.12', '-8.34', '+0.00']\n",
            "--------------------------------------------------\n",
            "Converged in 10 iterations.\n",
            "\n",
            "Optimal state values under uniform policy:\n",
            "\n",
            "['+0.00', '-7.83', '-11.12', '-12.23']\n",
            "['-7.83', '-10.42', '-11.77', '-11.86']\n",
            "['-11.12', '-11.77', '-11.05', '-8.81']\n",
            "['-12.23', '-11.86', '-8.81', '+0.00']\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the Gridworld\n",
        "grid_size = 4\n",
        "states = [(i, j) for i in range(grid_size) for j in range(grid_size)]\n",
        "actions = ['U', 'D', 'L', 'R']  # Up, Down, Left, Right\n",
        "gamma = 1.0\n",
        "theta = 1e-4  # Convergence threshold\n",
        "\n",
        "# Transition dynamics\n",
        "def step(state, action):\n",
        "    i, j = state\n",
        "    if state == (0, 0) or state == (3, 3):\n",
        "        return state, 0  # Terminal state\n",
        "\n",
        "    if action == 'U':\n",
        "        i = max(i - 1, 0)\n",
        "    elif action == 'D':\n",
        "        i = min(i + 1, grid_size - 1)\n",
        "    elif action == 'L':\n",
        "        j = max(j - 1, 0)\n",
        "    elif action == 'R':\n",
        "        j = min(j + 1, grid_size - 1)\n",
        "\n",
        "    return (i, j), -1\n",
        "\n",
        "# Initialize value function\n",
        "V = np.zeros((grid_size, grid_size))\n",
        "\n",
        "# Iterative Policy Evaluation\n",
        "iteration = 0\n",
        "while True:\n",
        "    delta = 0\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            state = (i, j)\n",
        "            if state == (0, 0) or state == (3, 3):\n",
        "                continue  # Skip terminal states\n",
        "\n",
        "            v = V[i, j]\n",
        "            new_v = 0\n",
        "            for action in actions:\n",
        "                next_state, reward = step(state, action)\n",
        "                ni, nj = next_state\n",
        "                new_v += 0.25 * (reward + gamma * V[ni, nj])  # uniform policy\n",
        "\n",
        "            V[i, j] = new_v\n",
        "            delta = max(delta, abs(v - new_v))\n",
        "    iteration += 1\n",
        "    if delta < theta:\n",
        "        break\n",
        "\n",
        "# Print the final value function\n",
        "print(f\"Converged in {iteration} iterations.\\n\")\n",
        "print(\"Optimal state values under uniform policy:\\n\")\n",
        "for row in V:\n",
        "    print([\"{:+.2f}\".format(val) for val in row])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhuF0qW74Xmo",
        "outputId": "d24c0b3b-5d53-4ff4-f44a-68c318e659e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged in 114 iterations.\n",
            "\n",
            "Optimal state values under uniform policy:\n",
            "\n",
            "['+0.00', '-14.00', '-20.00', '-22.00']\n",
            "['-14.00', '-18.00', '-20.00', '-20.00']\n",
            "['-20.00', '-20.00', '-18.00', '-14.00']\n",
            "['-22.00', '-20.00', '-14.00', '+0.00']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy = np.full((grid_size, grid_size), '', dtype='<U4')  # to store arrows\n",
        "\n",
        "action_map = {\n",
        "    'U': (-1, 0),\n",
        "    'D': (1, 0),\n",
        "    'L': (0, -1),\n",
        "    'R': (0, 1)\n",
        "}\n",
        "\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        state = (i, j)\n",
        "        if state == (0, 0) or state == (3, 3):\n",
        "            policy[i, j] = 'T'  # Terminal\n",
        "            continue\n",
        "\n",
        "        best_value = -float('inf')\n",
        "        best_action = None\n",
        "\n",
        "        for action in actions:\n",
        "            ni = max(0, min(i + action_map[action][0], grid_size - 1))\n",
        "            nj = max(0, min(j + action_map[action][1], grid_size - 1))\n",
        "            reward = -1\n",
        "            value = reward + gamma * V[ni, nj]\n",
        "\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = action\n",
        "\n",
        "        policy[i, j] = best_action\n",
        "\n",
        "\n",
        "print(\"\\nOptimal Policy (T = Terminal):\\n\")\n",
        "for row in policy:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0k_dUuEYqtk",
        "outputId": "cf398a8d-3857-4d92-dafd-6c608168c682"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimal Policy (T = Terminal):\n",
            "\n",
            "['T' 'L' 'L' 'L']\n",
            "['U' 'U' 'L' 'D']\n",
            "['U' 'U' 'R' 'D']\n",
            "['U' 'R' 'R' 'T']\n"
          ]
        }
      ]
    }
  ]
}