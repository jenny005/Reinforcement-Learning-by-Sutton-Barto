{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+uEfBijEx4jGQF/0Je1gO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenny005/Reinforcement-Learning-by-Sutton-Barto/blob/main/Chapter3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 3 : Finite Markov Decision Processes"
      ],
      "metadata": {
        "id": "-inYzFLVYAVG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGNEMZsPXV3c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gridworld"
      ],
      "metadata": {
        "id": "zF1zVBc_4WbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridworldEnv:\n",
        "    def __init__(self, height=5, width=5):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.action_space = [0, 1, 2, 3]  # 0: Up, 1: Down, 2: Left, 3: Right\n",
        "        self.A = (0, 1)\n",
        "        self.A_prime = (4, 1)\n",
        "        self.B = (0, 3)\n",
        "        self.B_prime = (2, 3)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        i, j = state\n",
        "\n",
        "        # Special teleportation states\n",
        "        if state == self.A:\n",
        "            return self.A_prime, 10\n",
        "        elif state == self.B:\n",
        "            return self.B_prime, 5\n",
        "\n",
        "        # Action to direction mapping\n",
        "        directions = {\n",
        "            0: (-1, 0),  # Up\n",
        "            1: (1, 0),   # Down\n",
        "            2: (0, -1),  # Left\n",
        "            3: (0, 1)    # Right\n",
        "        }\n",
        "\n",
        "        di, dj = directions[action]\n",
        "        ni, nj = i + di, j + dj\n",
        "\n",
        "        # Check boundary conditions\n",
        "        if 0 <= ni < self.height and 0 <= nj < self.width:\n",
        "            return (ni, nj), 0\n",
        "        else:\n",
        "            return (i, j), -1\n",
        "\n",
        "    def get_all_states(self):\n",
        "        return [(i, j) for i in range(self.height) for j in range(self.width)]\n",
        "\n",
        "    def get_available_actions(self, state):\n",
        "        return self.action_space\n"
      ],
      "metadata": {
        "id": "jpW0PrRh4Xjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridworldEnv()\n",
        "\n",
        "print(\"From A:\", env.step(env.A, 0))          # Expected: ((4, 1), 10)\n",
        "print(\"From B:\", env.step(env.B, 1))          # Expected: ((2, 3), 5)\n",
        "print(\"From (0, 0) going up:\", env.step((0, 0), 0))  # Expected: ((0, 0), -1)\n",
        "print(\"From (2, 2) going right:\", env.step((2, 2), 3))  # Expected: ((2, 3), 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnPdtjsY5VaC",
        "outputId": "1364e54f-4457-425f-9e81-f603460e0eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From A: ((4, 1), 10)\n",
            "From B: ((2, 3), 5)\n",
            "From (0, 0) going up: ((0, 0), -1)\n",
            "From (2, 2) going right: ((2, 3), 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the Gridworld\n",
        "grid_size = 4\n",
        "states = [(i, j) for i in range(grid_size) for j in range(grid_size)]\n",
        "actions = ['U', 'D', 'L', 'R']  # Up, Down, Left, Right\n",
        "gamma = 1.0\n",
        "theta = 1e-4  # Convergence threshold\n",
        "\n",
        "# Transition dynamics\n",
        "def step(state, action):\n",
        "    i, j = state\n",
        "    if state == (0, 0) or state == (3, 3):\n",
        "        return state, 0  # Terminal state\n",
        "\n",
        "    if action == 'U':\n",
        "        i = max(i - 1, 0)\n",
        "    elif action == 'D':\n",
        "        i = min(i + 1, grid_size - 1)\n",
        "    elif action == 'L':\n",
        "        j = max(j - 1, 0)\n",
        "    elif action == 'R':\n",
        "        j = min(j + 1, grid_size - 1)\n",
        "\n",
        "    return (i, j), -1\n",
        "\n",
        "# Initialize value function\n",
        "V = np.zeros((grid_size, grid_size))\n",
        "\n",
        "# Iterative Policy Evaluation\n",
        "iteration = 0\n",
        "while True:\n",
        "    delta = 0\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            state = (i, j)\n",
        "            if state == (0, 0) or state == (3, 3):\n",
        "                continue  # Skip terminal states\n",
        "\n",
        "            v = V[i, j]\n",
        "            new_v = 0\n",
        "            for action in actions:\n",
        "                next_state, reward = step(state, action)\n",
        "                ni, nj = next_state\n",
        "                new_v += 0.25 * (reward + gamma * V[ni, nj])  # uniform policy\n",
        "\n",
        "            V[i, j] = new_v\n",
        "            delta = max(delta, abs(v - new_v))\n",
        "    iteration += 1\n",
        "    if delta < theta:\n",
        "        break\n",
        "\n",
        "# Print the final value function\n",
        "print(f\"Converged in {iteration} iterations.\\n\")\n",
        "print(\"Optimal state values under uniform policy:\\n\")\n",
        "for row in V:\n",
        "    print([\"{:+.2f}\".format(val) for val in row])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhuF0qW74Xmo",
        "outputId": "0a8af0e5-b369-4b1d-a00d-e5dbf24b42b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged in 114 iterations.\n",
            "\n",
            "Optimal state values under uniform policy:\n",
            "\n",
            "['+0.00', '-14.00', '-20.00', '-22.00']\n",
            "['-14.00', '-18.00', '-20.00', '-20.00']\n",
            "['-20.00', '-20.00', '-18.00', '-14.00']\n",
            "['-22.00', '-20.00', '-14.00', '+0.00']\n"
          ]
        }
      ]
    }
  ]
}